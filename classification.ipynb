{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Definition_ **Regression** := when a statistical function returns a floating point value.\n",
    "\n",
    "_Definition_ **Classification** := when a statistical function returns a category.  Like a boolean or a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.591023</td>\n",
       "      <td>16</td>\n",
       "      <td>90.296527</td>\n",
       "      <td>'molly'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.626541</td>\n",
       "      <td>13</td>\n",
       "      <td>89.357373</td>\n",
       "      <td>'dolly'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.121086</td>\n",
       "      <td>23</td>\n",
       "      <td>95.636287</td>\n",
       "      <td>'molly'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.691377</td>\n",
       "      <td>19</td>\n",
       "      <td>95.360784</td>\n",
       "      <td>'molly'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.403012</td>\n",
       "      <td>5</td>\n",
       "      <td>96.244526</td>\n",
       "      <td>'dolly'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a   b          c        d\n",
       "0 -0.591023  16  90.296527  'molly'\n",
       "1  0.626541  13  89.357373  'dolly'\n",
       "2 -0.121086  23  95.636287  'molly'\n",
       "3  0.691377  19  95.360784  'molly'\n",
       "4 -0.403012   5  96.244526  'dolly'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"assignment_files/data.csv\")\n",
    "columns = [elem for elem in df.columns.tolist() if \"Unnamed\" not in elem]\n",
    "df = df[columns]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.968053216159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.508"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "logit = linear_model.LogisticRegression()\n",
    "model = logit.fit(X, y)\n",
    "predicted = model.predict(X)\n",
    "unique_elems = list(set(y))\n",
    "mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "predicted = [mapping[elem] for elem in predicted]\n",
    "y = [mapping[elem] for elem in y]\n",
    "print(metrics.r2_score(y, predicted))\n",
    "success_count = 0\n",
    "for index, elem in enumerate(y):\n",
    "    if predicted[index] == elem:\n",
    "        success_count += 1\n",
    "success_count / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import random\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "decision_tree =tree.DecisionTreeClassifier()\n",
    "model = decision_tree.fit(X, y)\n",
    "predicted = model.predict(X)\n",
    "unique_elems = list(set(y))\n",
    "mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "predicted = [mapping[elem] for elem in predicted]\n",
    "y = [mapping[elem] for elem in y]\n",
    "print(metrics.accuracy_score(y, predicted))\n",
    "success_count = 0\n",
    "for index, elem in enumerate(y):\n",
    "    if predicted[index] == elem:\n",
    "        success_count += 1\n",
    "print(success_count / len(y))\n",
    "random_index = random.randint(0,len(y))\n",
    "y[random_index] == predicted[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that we make use of `accuracy_score` instead of `r_2` for the decision tree.  That's because `r_2` assumes a linear model.  It is possible for `r_2` to not be able produce accurate results for binary or multiclass classification.  \n",
    "\n",
    "Now that we have a baseline for dealing with numbers, let's look at some more models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of KNN with a k of 1 is 1.0\n",
      "The accuracy of KNN with a k of 2 is 0.7545\n",
      "The accuracy of KNN with a k of 3 is 0.7504\n",
      "The accuracy of KNN with a k of 4 is 0.6857\n",
      "The accuracy of KNN with a k of 5 is 0.6858\n",
      "The accuracy of KNN with a k of 6 is 0.6603\n",
      "The accuracy of KNN with a k of 7 is 0.6601\n",
      "The accuracy of KNN with a k of 8 is 0.6421\n",
      "The accuracy of KNN with a k of 9 is 0.6359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "clfs = []\n",
    "fit_scores = []\n",
    "for i in range(1,10):\n",
    "    clfs.append(KNeighborsClassifier(n_neighbors=i))\n",
    "for clf in clfs:\n",
    "    model = clf.fit(X, y)\n",
    "    predicted = model.predict(X)\n",
    "    unique_elems = list(set(y))\n",
    "    mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "    predicted_vals = [mapping[elem] for elem in predicted]\n",
    "    y_vals = [mapping[elem] for elem in y]\n",
    "    fit_scores.append(metrics.accuracy_score(y_vals, predicted_vals))\n",
    "for index,i in enumerate(list(range(1,10))):\n",
    "    print(\"The accuracy of KNN with a k of {} is {}\".format(i, fit_scores[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that with a k of two the data is fit the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6269\n",
      "0.6269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import random\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "clf = SVC(random_state=0, kernel='rbf', probability=True)\n",
    "model = clf.fit(X, y)\n",
    "predicted = model.predict(X)\n",
    "unique_elems = list(set(y))\n",
    "mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "predicted = [mapping[elem] for elem in predicted]\n",
    "y = [mapping[elem] for elem in y]\n",
    "print(metrics.accuracy_score(y, predicted))\n",
    "success_count = 0\n",
    "for index, elem in enumerate(y):\n",
    "    if predicted[index] == elem:\n",
    "        success_count += 1\n",
    "print(success_count / len(y))\n",
    "random_index = random.randint(0,len(y))\n",
    "y[random_index] == predicted[random_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a wide range of classification tasks, usually SVM performs quiet well.  However, it does not here.  We have now seen a set of classification algorithms.  Now we are ready to look at ensemble approaches - combining many weak classifiers to create a strong classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "0.6857\n",
      "svm\n",
      "0.6269\n",
      "svm_gamma\n",
      "0.785\n",
      "rf\n",
      "0.9763\n",
      "Ensemble\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics, feature_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "\n",
    "# Plotting Decision Regions\n",
    "labels = ['Logistic Regression',\n",
    "          'Random Forest',\n",
    "          'RBF kernel SVM',\n",
    "          'Ensemble']\n",
    "\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=7) #strong\n",
    "clf2 = SVC(random_state=0, kernel='rbf', probability=True) #strong\n",
    "clf3 = SVC(gamma=2, C=1, probability=True) #strong\n",
    "clf4 = RandomForestClassifier(random_state=0) #strong\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('knn', clf1), ('svm', clf2),\n",
    "    ('svm_gamma', clf3), ('rf', clf4), \n",
    "]\n",
    "\n",
    "eclf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "params = {\n",
    "    'rf__n_estimators': [20, 50, 100],\n",
    "    'knn__n_neighbors': [2,3,4,5,6,7,8,9,10,11,12],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "\n",
    "for label, clf in estimators:\n",
    "    print(label)\n",
    "    if label == 'knn':\n",
    "        params = {\"n_neighbors\": [2,3,4,5,6,7,8,9,10,11,12]}\n",
    "        clf = GridSearchCV(estimator=clf, param_grid=params, cv=5)\n",
    "    model = clf.fit(X, y)\n",
    "    predicted = model.predict(X)\n",
    "    unique_elems = list(set(y))\n",
    "    mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "    predicted_vals = [mapping[elem] for elem in predicted]\n",
    "    y_vals = [mapping[elem] for elem in y]\n",
    "    print(metrics.accuracy_score(y_vals, predicted_vals))\n",
    "\n",
    "    \n",
    "print(\"Ensemble\")\n",
    "model = grid.fit(X, y)\n",
    "predicted = model.predict(X)\n",
    "unique_elems = list(set(y))\n",
    "mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "predicted_vals = [mapping[elem] for elem in predicted]\n",
    "y_vals = [mapping[elem] for elem in y]\n",
    "print(metrics.accuracy_score(y_vals, predicted_vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some more ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Setting: {'learning_rate': 1.0, 'subsample': 1.0}\n",
      "1.0\n",
      "GBT Setting: {'learning_rate': 0.1, 'subsample': 1.0}\n",
      "0.9583\n",
      "GBT Setting: {'learning_rate': 1.0, 'subsample': 0.5}\n",
      "0.706\n",
      "GBT Setting: {'learning_rate': 0.1, 'subsample': 0.5}\n",
      "0.9298\n",
      "GBT Setting: {'learning_rate': 0.1, 'max_features': 2}\n",
      "0.9637\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "\n",
    "original_params = {'n_estimators': 1000, 'max_leaf_nodes': 17, 'max_depth': None, 'random_state': 2,\n",
    "                   'min_samples_split': 5}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for label, color, setting in [('No shrinkage', 'orange',\n",
    "                               {'learning_rate': 1.0, 'subsample': 1.0}),\n",
    "                              ('learning_rate=0.1', 'turquoise',\n",
    "                               {'learning_rate': 0.1, 'subsample': 1.0}),\n",
    "                              ('subsample=0.5', 'blue',\n",
    "                               {'learning_rate': 1.0, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, subsample=0.5', 'gray',\n",
    "                               {'learning_rate': 0.1, 'subsample': 0.5}),\n",
    "                              ('learning_rate=0.1, max_features=2', 'magenta',\n",
    "                               {'learning_rate': 0.1, 'max_features': 2})]:\n",
    "    params = dict(original_params)\n",
    "    params.update(setting)\n",
    "\n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    model = clf.fit(X, y)\n",
    "    predicted = model.predict(X)\n",
    "    unique_elems = list(set(y))\n",
    "    mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "    predicted_vals = [mapping[elem] for elem in predicted]\n",
    "    y_vals = [mapping[elem] for elem in y]\n",
    "    print(\"GBT Setting:\", setting)\n",
    "    print(metrics.accuracy_score(y_vals, predicted_vals))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at a neural network!  Our very first :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# larger model\n",
    "def create_network():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(35, input_dim=3, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(17, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "X = df[[\"a\", \"b\", \"c\"]]\n",
    "y = df[\"d\"]\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_network, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "#kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "model = pipeline.fit(X, y)\n",
    "predicted = model.predict(X)\n",
    "unique_elems = list(set(y))\n",
    "mapping = {\"'dolly'\":0, \"'molly'\":1}\n",
    "predicted_vals = [mapping[elem] for elem in predicted]\n",
    "y_vals = [mapping[elem] for elem in y]\n",
    "print(\"GBT Setting:\", setting)\n",
    "print(metrics.accuracy_score(y_vals, predicted_vals))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we'll be looking at is an application of classification called text classification, where we create high level labels for text.  Below is an example - don't worry if it doesn't make sense!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "def train_classifier(text,labels):\n",
    "    #http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#parameter-tuning-using-grid-search\n",
    "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf__alpha': (1e-2, 1e-3),}\n",
    "    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(tokenizer=LemmaTokenizer())),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                               alpha=1e-3, n_iter=5, random_state=42)),])\n",
    "\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "    return gs_clf.fit(text,labels)\n",
    "        \n",
    "def classify_text(classifier,input_data):\n",
    "    return classifier.predict([input_data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greeting'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"hello there\", \"hi there\", \"hello\", \"bye\", \"goodbye\", \"seeya\"]\n",
    "labels = [\"greeting\", \"greeting\", \"greeting\", \"later\", \"later\", \"later\"]\n",
    "clf = train_classifier(text, labels)\n",
    "classify_text(clf, \"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
